{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WIldan Miftakhurahman\n",
    "### 21110040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File teks telah berhasil dibuat di: output.txt\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Baca file CSV\n",
    "csv_file_path = 'dataset_mentalhealth.csv'\n",
    "\n",
    "with open(csv_file_path, 'r', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    \n",
    "    # Tentukan kolom yang ingin diambil\n",
    "    kolom_jawaban = 'Jawaban'\n",
    "    \n",
    "    # Buat list untuk menyimpan nilai kolom Jawaban\n",
    "    jawaban_list = [row[kolom_jawaban] for row in csv_reader]\n",
    "\n",
    "# Buat file teks dengan nilai kolom Jawaban\n",
    "txt_file_path = 'output.txt'\n",
    "\n",
    "with open(txt_file_path, 'w', encoding='utf-8') as txt_file:\n",
    "    for jawaban in jawaban_list:\n",
    "        txt_file.write(jawaban + '\\n')\n",
    "\n",
    "print(f'File teks telah berhasil dibuat di: {txt_file_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import random\n",
    "import string\n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk #impor library NLTK\n",
    "from nltk.stem import WordNetLemmatizer #import library untuk lemmatization\n",
    "nltk.download('popular', quiet=True) # for downloading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('output.txt','r',errors = 'ignore') #membuka file corpus dari wikipedia\n",
    "raw=f.read() #raw kini berisi semua data dari corpus per baris (raw)\n",
    "raw = raw.lower()# converts to lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenisasi adalah memilah-milah dokumen ke kalimat-kalimat,\n",
    "#kemudian memilah setiap kalimat menjadi sekumpulan kata kata\n",
    "sent_tokens = nltk.sent_tokenize(raw) # converts dokumen corpus ke kalimat-kalimat\n",
    "word_tokens = nltk.word_tokenize(raw)# converts dokumen corpus ke kata-kata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
    "def LemTokens(tokens):\n",
    " return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    " return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kata-kata pembuka didaftar terlebih dulu dan kemudian secara acak diberikan respon jawabannya\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\", \"hai\")\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"hi there\", \"hello\"]\n",
    "def greeting(sentence):\n",
    " for word in sentence.split():\n",
    "    if word.lower() in GREETING_INPUTS:\n",
    "        return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling dengan TF-IDF dan Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    robo_response='' #pada tahap awal respon mesin diisi karakter kosong\n",
    "    sent_tokens.append(user_response) #pertanyaan user ditokenisasi dan ditambahkan di corpus\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='indonesian')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens) #token dari pertanyaan user di vektorisasi\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf) #hitung similarity setiap token corspus dengan token pertanyaan\n",
    "    idx=vals.argsort()[0][-2] #sort jarak similariti setiap token corpus dengan token pertanyaan\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0): #jika pertanyaan dan semua token corpus jaraknya tinggi maka\n",
    "    #berarti pertanyaan tidak ada jawabannya\n",
    "        robo_response=robo_response+\"Mohon maaf, saya tidak paham pertanyaan anda\"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx] #jika jaraknya terrendah maka dipakai sebagai jawaban\n",
    "        return robo_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uji Coba / Evaluasi Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================== \n",
      " Mesin: Ini adalah layanan autochat, untuk mengakhiri sesi, ketik selesai\n",
      "user : adakah kelompok pendukung bagi penyandang gangguan mental?\n",
      "Mesin: ada kelompok pendukung untuk orang -orang dengan pengalaman penyakit mental, kelompok pendukung untuk orang dengan diagnosis khusus, kelompok pendukung untuk anggota keluarga dan teman, dan banyak lagi.\n",
      "user : apa yang dimaksud dengan bersepeda cepat?\n",
      "Mesin: bersepeda yang cepat dapat terjadi kapan saja seseorang mengalami gangguan bipolar-sekitar 10-20% orang yang didiagnosis dengan gangguan bipolar mengalami bersepeda cepat di beberapa titik.\n",
      "user : apa perbedaan kesehatan mental vs penyakit mental?\n",
      "Mesin: sama seperti itu mungkin untuk memiliki kesehatan mental yang buruk tetapi tidak ada penyakit mental, sangat mungkin untuk memiliki kesehatan mental yang baik bahkan dengan diagnosis penyakit mental.\n",
      "user : apa saja sisi buruk asap ganja?\n",
      "Mesin: asap ganja, misalnya, mengandung racun penyebab kanker.\n",
      "user : apakah psikosis dapat diobati?\n",
      "Mesin: psikosis dan skizofrenia dapat diobati.\n",
      "user : penyakit mental adalah\n",
      "Mesin: sama seperti itu mungkin untuk memiliki kesehatan mental yang buruk tetapi tidak ada penyakit mental, sangat mungkin untuk memiliki kesehatan mental yang baik bahkan dengan diagnosis penyakit mental.\n",
      "user : definisi vaping\n",
      "Mesin: \"vaping\" adalah istilah untuk menggunakan perangkat di mana cairan, sering dibumbui, diubah menjadi uap (karenanya, vaping) dan dihirup.\n",
      "user : status minyak cbd?\n",
      "Mesin: setelah meneliti keamanan dan efektivitas minyak cbd untuk mengobati epilepsi, pada tahun 2018, fda amerika serikat menyetujui cbd (epidiolex) sebagai terapi untuk dua kondisi langka yang ditandai dengan kejang epilepsi.\n",
      "user : prodrome adalah\n",
      "Mesin: psikosis, sekelompok gejala yang ditemukan dalam gangguan seperti skizofrenia, adalah satu penyakit dengan prodrome tertentu.\n",
      "user : gangguan jiwa identitas adalah\n",
      "Mesin: gangguan identitas disosiatif sebenarnya lebih tentang identitas yang terfragmentasi daripada banyak kepribadian berbeda yang berkembang sendiri.\n",
      "user : saya suka main game\n",
      "Mesin: bos saya memberi tahu saya bahwa saya melakukan kesalahan.\n",
      "user : bandung lautan api\n",
      "Mesin: Mohon maaf, saya tidak paham pertanyaan anda\n",
      "user : selesai\n",
      "Mesin: Terimakasih sudah menggunakna layanan chatbot\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "print(\"======================== \\n Mesin: Ini adalah layanan autochat, untuk mengakhiri sesi, ketik selesai\")\n",
    "while(flag==True):\n",
    "    user_response = input(\"Masukkan pertanyaan :\")\n",
    "    user_response=user_response.lower()\n",
    "    print('user :', user_response)\n",
    "    if(user_response!='selesai'): #jika user tidak keluar\n",
    "        if(user_response=='thanks' or user_response=='thank you' ): #jika ucapkan thanks/thankyou\n",
    "            flag=False #tandai proses berhenti\n",
    "            print(\"Mesin: You are welcome..\") #balasan thank you\n",
    "        else:\n",
    "            if(greeting(user_response)!=None): #jika response adalah kalimat greeting\n",
    "                print(\"Mesin: \"+greeting(user_response)) #tampilkan kalimat greeting \n",
    "            else: #jika bukan kalimat greeting\n",
    "                print(\"Mesin: \",end=\"\")\n",
    "                print(response(user_response)) #memproses user answer disini\n",
    "                sent_tokens.remove(user_response) #user answer dihapus setelah dimunculkan \n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"Mesin: Terimakasih sudah menggunakna layanan chatbot\")\n",
    "        print(\"========================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluasi Pertanyaan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model dapat dianggap sukses karena memberikan jawaban yang relevan dan informatif dalam banyak kasus. Namun, preprocessing yang digunakan masih membiarkan banyak stopwords yang lolos, sehingga nilai TF-IDF kurang optimal. Ini dapat mengakibatkan jawaban yang kurang spesifik dan tidak sepenuhnya memanfaatkan informasi yang tersedia.\n",
    "\n",
    "Selain itu, terdapat kasus di mana input pengguna dan jawaban mesin tidak sesuai. Misalnya, pada pertanyaan \"saya suka main game\", jawaban mesin tidak relevan dan mungkin disebabkan oleh ketidaksesuaian model terhadap jenis pertanyaan tersebut. Hal ini menunjukkan bahwa model mungkin perlu peningkatan untuk menangani variasi jenis pertanyaan dengan lebih baik.\n",
    "\n",
    "Pada bagian akhir, pada pertanyaan \"bandung lautan api\", jawaban mesin tidak relevan dan menyebutkan kesalahan yang tidak terkait. Ini menunjukkan bahwa model tidak sepenuhnya memahami jenis pertanyaan tersebut dan memutuskan bahwa pertanyaan tidak sesuai sehingga memutuskan untuk mengeluarkan ourput \"Mohon Maaf, saya tidak paham pertanyaan anda\".yang bisa dipahami karna memang didataset pertanyaan seperti itu tidak ada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
